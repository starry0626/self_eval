"""
evaluate_model_official_standard.py: ä¸¥æ ¼éµå¾ªQwen3-VLå®˜æ–¹å¤„ç†æµç¨‹
"""
import sys
import json
import re
import torch
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from pathlib import Path
from typing import Dict, List, Tuple, Optional, Any
from tqdm import tqdm
from PIL import Image
import time

from transformers import Qwen3VLForConditionalGeneration, AutoProcessor

# ==================== ä¸¥æ ¼å¯¼å…¥å®˜æ–¹å·¥å…· ====================
try:
    from qwen_vl_utils import process_vision_info
    print("âœ“ Imported official process_vision_info")
except ImportError:
    sys.path.append(str(Path(__file__).parent / "qwen-vl-utils"))
    from qwen_vl_utils import process_vision_info
    print("âœ“ Imported local process_vision_info")

VIDEO_GROUNDED_QA_PROMPT = """You are a video understanding assistant. Please analyze the provided video and answer the multiple-choice question.

IMPORTANT: You MUST follow this exact format:
1. First, enclose your step-by-step thinking process within <think> tags
2. Then provide one or more relevant time ranges (in seconds) that support your answer, each enclosed in <time_range> tags
3. Finally, provide your final answer choice enclosed in <answer> tags

Required format:
<think>Your detailed reasoning process here...</think>
<time_range>start_time1, end_time1</time_range>
<time_range>start_time2, end_time2</time_range>  (if there are multiple relevant segments)
<answer>A/B/C/D</answer>

Question: {question}
Options:
{options}

Note: The video duration is {duration} seconds. Sample frames are provided from key moments. There may be multiple relevant time periods in the video."""

# ==================== é…ç½®å‚æ•° ====================
USE_FLASH_ATTENTION = True
USE_TORCH_COMPILE = False
DTYPE = torch.bfloat16

def load_model_with_acceleration(model_path: str, device: str = "auto"):
    """åŠ è½½æ¨¡å‹å¹¶åº”ç”¨åŠ é€Ÿä¼˜åŒ–"""
    print("Loading Qwen3-VL model with acceleration optimizations...")
    
    # ==================== ä¿®å¤1ï¼šè½¬æ¢è·¯å¾„ä¸ºç»å¯¹è·¯å¾„ ====================
    model_path = str(Path(model_path).resolve())  # è½¬æ¢ä¸ºç»å¯¹è·¯å¾„
    
    load_config = {
        "torch_dtype": DTYPE,  # æš‚æ—¶ä¿ç•™ï¼Œåé¢å…¼å®¹å¤„ç†
        "device_map": device,
        "trust_remote_code": True,
    }
    
    # å…¼å®¹æ–°æ—§ç‰ˆtransformers
    try:
        # æ–°ç‰ˆä½¿ç”¨dtypeå‚æ•°
        from packaging import version
        import transformers
        if version.parse(transformers.__version__) >= version.parse("4.45.0"):
            load_config["dtype"] = DTYPE
            del load_config["torch_dtype"]
    except:
        pass
    
    if USE_FLASH_ATTENTION:
        try:
            load_config["attn_implementation"] = "flash_attention_2"
            print("âœ“ Flash Attention 2 enabled")
        except:
            print("âš  Flash Attention 2 not available, using default attention")
    
    # ==================== ä¿®å¤2ï¼šç¡®ä¿è·¯å¾„æœ‰æ•ˆ ====================
    model_path = Path(model_path)
    if not model_path.exists():
        raise FileNotFoundError(f"Model path not found: {model_path}")
    
    # from_pretrainedå¯ä»¥æ¥å—stræˆ–Pathå¯¹è±¡
    model = Qwen3VLForConditionalGeneration.from_pretrained(
        str(model_path),  # ç¡®ä¿ä¼ å…¥å­—ç¬¦ä¸²è·¯å¾„
        **load_config
    )
    
    if USE_TORCH_COMPILE:
        print("Applying torch.compile (this may take a few minutes on first run)...")
        model = torch.compile(model, mode="reduce-overhead")
    
    processor = AutoProcessor.from_pretrained(
        str(model_path),  # åŒæ ·ä½¿ç”¨è§„èŒƒè·¯å¾„
        trust_remote_code=True
    )
    
    return model, processor


def evaluate_model_on_dataset(
    model_path: str,
    dataset_path: str,
    output_dir: str,
    max_samples: Optional[int] = None,
    device: str = "auto",
    max_frames: int = 16,
    batch_size: int = 1
) -> pd.DataFrame:
    """
    åœ¨æ•°æ®é›†ä¸Šè¯„ä¼°æ¨¡å‹å¹¶è®¡ç®—å¥–åŠ±ï¼ˆä¸¥æ ¼å®˜æ–¹æµç¨‹ï¼‰
    """
    model, processor = load_model_with_acceleration(model_path, device)
    
    print(f"Loading dataset from {dataset_path}...")
    with open(dataset_path, 'r', encoding='utf-8') as f:
        dataset = json.load(f)
    
    if max_samples:
        dataset = dataset[:max_samples]
    
    output_path = Path(output_dir)
    output_path.mkdir(parents=True, exist_ok=True)
    
    results = []
    inference_times = []
    
    print(f"Evaluating {len(dataset)} samples...")
    
    for idx, sample in enumerate(tqdm(dataset, desc="Evaluating")):
        start_time = time.time()
        
        try:
            # ==================== æ ¸å¿ƒï¼šä½¿ç”¨æ ‡å‡†å®˜æ–¹æµç¨‹ ====================
            result = process_single_sample_official(
                sample, model, processor, max_frames
            )
            if result:
                results.append(result)
                
                inference_time = time.time() - start_time
                inference_times.append(inference_time)
                
                if (idx + 1) % 10 == 0:
                    avg_time = np.mean(inference_times[-10:])
                    print(f"  Average inference time (last 10): {avg_time:.2f}s")
                    
        except Exception as e:
            print(f"âŒ Error processing sample {idx} (ID: {sample.get('problem_id')}): {e}")
            import traceback
            traceback.print_exc()
            continue
    
    results_df = pd.DataFrame(results)
    results_df.to_json(output_path / "evaluation_results.json", orient="records", indent=2)
    results_df.to_csv(output_path / "evaluation_results.csv", index=False)
    
    if inference_times:
        time_stats = {
            "mean_inference_time": float(np.mean(inference_times)),
            "std_inference_time": float(np.std(inference_times)),
            "total_inference_time": float(np.sum(inference_times)),
            "samples_per_second": len(inference_times) / np.sum(inference_times)
        }
        with open(output_path / "inference_stats.json", 'w') as f:
            json.dump(time_stats, f, indent=2)
        
        print(f"\nInference speed: {time_stats['samples_per_second']:.2f} samples/sec")
    
    print(f"âœ“ Evaluation completed. Results saved to {output_path}")
    return results_df


def process_single_sample_official(
    sample: Dict, 
    model: Any, 
    processor: Any, 
    max_frames: int
) -> Optional[Dict]:
    """ä¸¥æ ¼éµå¾ªå®˜æ–¹ç¤ºä¾‹çš„å¤„ç†æµç¨‹"""
    video_path = sample["path"]
    
    # ==================== æ­¥éª¤1: æ„é€ æ ‡å‡†messages ====================
    options_text = "\n".join(sample["options"])
    prompt = VIDEO_GROUNDED_QA_PROMPT.format(
        question=sample["problem"],
        options=options_text,
        duration=sample["duration"]
    )
    
    # å®˜æ–¹æ ¼å¼çš„messagesï¼ˆå…³é”®æ”¹åŠ¨ï¼‰
    messages = [
        {
            "role": "user",
            "content": [
                {
                    "type": "video",
                    "video": video_path,  # ç›´æ¥ä¼ è·¯å¾„ï¼Œç”±å®˜æ–¹å·¥å…·å¤„ç†
                    # å¯ä»¥åœ¨è¿™é‡ŒæŒ‡å®šè§†é¢‘å¤„ç†å‚æ•°ï¼ˆå¦‚æœprocess_vision_infoæ”¯æŒï¼‰
                    # "max_frames": max_frames,
                    "fps": 0.5,
                },
                {
                    "type": "text",
                    "text": prompt
                }
            ]
        }
    ]
    
    # ==================== æ­¥éª¤2: åº”ç”¨chat template ====================
    text = processor.apply_chat_template(
        messages,
        tokenize=False,
        add_generation_prompt=True
    )
    
    # ==================== æ­¥éª¤3: å¤„ç†è§†è§‰ä¿¡æ¯ï¼ˆæ ¸å¿ƒå®˜æ–¹å‡½æ•°ï¼‰ ====================
    try:
        # process_vision_infoä¼šè‡ªåŠ¨å¤„ç†è§†é¢‘è§£ç ã€é‡‡æ ·ã€æ—¶é—´æˆ³
        images, videos, video_kwargs = process_vision_info(
            messages,
            image_patch_size=16,
            # max_frames=max_frames,  # å‡è®¾process_vision_infoæ”¯æŒæ­¤å‚æ•°
            return_video_kwargs=True,
            return_video_metadata=True
        )
    except Exception as e:
        print(f"âš ï¸  process_vision_info failed for {video_path}: {e}")
        return None
    print('=============videos============= ', '\n', videos)
    print('=============video_kwargs=============', '\n', video_kwargs)
    # ==================== æ­¥éª¤4: å¤„ç†è§†é¢‘å…ƒæ•°æ® ====================
    video_metadata = None
    if videos is not None:
        # è§£å‹videoså’Œmetadatasï¼ˆä¸å®˜æ–¹ç¤ºä¾‹å®Œå…¨ä¸€è‡´ï¼‰
        videos, video_metadatas = zip(*videos)
        videos, video_metadata = list(videos), list(video_metadatas)
        
        print(f"âœ“ Video processed: {len(videos)} frames with metadata")
        print(f"  Video kwargs: {video_kwargs}")
    
    # ==================== æ­¥éª¤5: æ„é€ processorè¾“å…¥ ====================
    # ä¸¥æ ¼éµå¾ªå®˜æ–¹ç¤ºä¾‹çš„å‚æ•°
    inputs = processor(
        text=text,
        images=images,
        videos=videos,
        video_metadata=video_metadata,
        return_tensors="pt",
        do_resize=False,  # å®˜æ–¹ç¤ºä¾‹æ˜ç¡®è®¾ç½®
        # å…¶ä»–å‚æ•°ç”±video_kwargsè‡ªåŠ¨ä¼ é€’
        **video_kwargs
    )
    print("inputs keys=", list(inputs.keys()))# inputs keys= ['input_ids', 'attention_mask', 'pixel_values_videos', 'video_grid_thw']
    print("inputs['input_ids'] shape= ", inputs['input_ids'].shape)
    print("inputs['pixel_values_videos'] shape= ", inputs['pixel_values_videos'].shape)
    print("inputs['attention_mask'] shape= ", inputs['attention_mask'].shape)
    # print("inputs['video_grid_thw']= ", inputs['video_grid_thw'])
    print("inputs['video_grid_thw']shape= ", inputs['video_grid_thw'].shape)
    # print('=============inputs============= ', '\n', inputs)
    # inputs = inputs.to(model.device)
    
    # ==================== æ­¥éª¤6: æ¨¡å‹æ¨ç† ====================
    # with torch.no_grad():
    #     generation_config = {
    #         "max_new_tokens": 512,
    #         "do_sample": False,
    #         "temperature": 0.0,
    #         "use_cache": True,
    #     }
        
    #     generated_ids = model.generate(**inputs, **generation_config)
    
    # # ==================== æ­¥éª¤7: è§£ç è¾“å‡º ====================
    # generated_ids_trimmed = generated_ids[:, inputs.input_ids.shape[1]:]
    # response = processor.batch_decode(
    #     generated_ids_trimmed,
    #     skip_special_tokens=True,
    #     clean_up_tokenization_spaces=False
    # )[0]
    
    # # ==================== æ­¥éª¤8: è®¡ç®—å¥–åŠ± ====================
    # accuracy_reward = calculate_accuracy(response, sample["solution"])
    # pred_time_ranges = extract_multiple_time_ranges(response)
    # temporal_iou = calculate_temporal_iou_multi(pred_time_ranges, sample["relevant_windows"])
    # format_reward = calculate_format_reward(response)
    # total_reward = 0.4 * accuracy_reward + 0.4 * temporal_iou + 0.2 * format_reward
    
    # # æå–å®é™…ä½¿ç”¨çš„æ—¶é—´æˆ³ï¼ˆä»video_metadataï¼‰
    # actual_timestamps = []
    # if video_metadata and len(video_metadata) > 0:
    #     # video_metadataä¸­é€šå¸¸åŒ…å«frame_secondsä¿¡æ¯
    #     actual_timestamps = video_metadata[0].get("frame_seconds", [])
    
    # return {
    #     "problem_id": sample["problem_id"],
    #     "question": sample["problem"],
    #     "ground_truth_solution": sample["solution"],
    #     "ground_truth_windows": sample["relevant_windows"],
    #     "model_response": response,
    #     "predicted_time_ranges": pred_time_ranges,
    #     "video_timestamps_used": actual_timestamps,
    #     "video_frames_count": len(videos) if videos else 0,
    #     "video_kwargs_used": video_kwargs,  # è®°å½•ä½¿ç”¨çš„å‚æ•°
    #     "accuracy_reward": accuracy_reward,
    #     "temporal_iou_reward": temporal_iou,
    #     "format_reward": format_reward,
    #     "total_reward": total_reward,
    #     "data_source": sample.get("data_source", ""),
    #     "video_path": video_path
    # }




def verify_local_utils():
    """éªŒè¯æœ¬åœ°qwen-vl-utilsæ˜¯å¦å¯ç”¨"""
    print("ğŸ” Verifying local qwen-vl-utils installation...")
    
    try:
        from qwen_vl_utils import process_vision_info
        print("âœ“ qwen_vl_utils.process_vision_info imported successfully")
        return True
    except ImportError as e:
        print(f"âŒ Import failed: {e}")
        return False


def main():
    # ==================== é…ç½®å‚æ•° ====================
    MODEL_PATH = "Qwen3-VL-2B-Thinking"   # æœ¬åœ°æ¨¡å‹è·¯å¾„
    DATASET_PATH = "./dataset/video-mtr/qv-nextgqa_merge_8k.json"    # æ•°æ®é›†è·¯å¾„
    OUTPUT_DIR = "./evaluation_results_offical_standard"
    MAX_SAMPLES = 1  # è¯„ä¼°æ ·æœ¬æ•°
    MAX_FRAMES = 64   # æ¯è§†é¢‘æœ€å¤§é‡‡æ ·å¸§æ•°
    
    # éªŒè¯æœ¬åœ°å·¥å…·
    if not verify_local_utils():
        print("\nâš ï¸  Please ensure qwen-vl-utils is installed locally:")
        print("   git clone https://github.com/QwenLM/qwen-vl-utils.git")
        print("   cd qwen-vl-utils && pip install -e .")
        return
    
    # è¿è¡Œè¯„ä¼°
    results_df = evaluate_model_on_dataset(
        model_path=MODEL_PATH,
        dataset_path=DATASET_PATH,
        output_dir=OUTPUT_DIR,
        max_samples=MAX_SAMPLES,
        max_frames=MAX_FRAMES,
        batch_size=1
    )
    
    # ç»˜åˆ¶åˆ†å¸ƒå›¾
    # plot_reward_distributions(results_df, OUTPUT_DIR)
    
    # print("âœ… All done! Strictly following official Qwen3-VL processing pipeline.")


if __name__ == "__main__":
    main()